<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NeuroScan Pro - Detailed Presentation</title>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/reset.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/reveal.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/theme/black.min.css" id="theme">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/monokai.min.css">

    <style>
        /* --- CUSTOM THEME --- */
        :root { --primary: #42affa; --secondary: #ff7f50; }
        .reveal { font-size: 28px; } /* Slightly smaller base font for more details */
        .reveal h1 { font-size: 2.2em; text-transform: none; color: #fff; }
        .reveal h2 { font-size: 1.6em; text-transform: none; color: #fff; border-bottom: 2px solid var(--primary); display: inline-block; padding-bottom: 10px; }
        .reveal h3 { font-size: 1.2em; color: #ccc; }
        
        .highlight { color: var(--primary); font-weight: bold; }
        .accent { color: var(--secondary); font-weight: bold; }

        /* Layouts */
        .cols { display: flex; justify-content: space-between; align-items: flex-start; gap: 40px; height: 100%; }
        .col { flex: 1; text-align: left; }
        
        /* Image Containers */
        .img-box {
            background: rgba(255, 255, 255, 0.05);
            border: 1px solid #555;
            border-radius: 10px;
            padding: 10px;
            text-align: center;
        }
        .reveal img { max-height: 400px; border: none; box-shadow: none; background: transparent; }

        /* Small Technical Notes */
        .tech-note { font-size: 0.7em; color: #aaa; margin-top: 10px; line-height: 1.4; border-left: 3px solid var(--primary); padding-left: 10px; }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">

            <section>
                <h1>ü©ª NeuroScan Pro</h1>
                <h3>Multi-Model Anomaly Detection in Medical Imaging</h3>
                <p>Advanced Unsupervised Pathology Detection using <br> <span class="highlight">VAE, GAN, and Vision Transformers</span></p>
                
                <div style="margin-top: 40px; border-top: 1px solid #555; padding-top: 20px;">
                    <p style="font-size: 0.8em; color: #888; letter-spacing: 2px; text-transform: uppercase;">Project Team</p>
                    <p style="font-size: 1em; font-weight: bold;">Harshal Zope &bull; Anirudha Deshmukh &bull; Kiran Gavali</p>
                </div>
            </section>

            <section>
                <h2>‚ùì The Problem: "Black Box" Failure</h2>
                <div class="cols">
                    <div class="col">
                        <ul style="font-size: 0.9em;">
                            <li class="fragment fade-up"><strong>Data Scarcity & Privacy:</strong> Medical data is highly regulated (HIPAA). "Normal" X-rays are abundant, but specific disease samples are rare and expensive to annotate.</li>
                            <li class="fragment fade-up"><strong>The "Black Swan" Limit:</strong> Supervised classifiers (e.g., ResNet) only detect what they are trained on. If a new virus or rare tumor appears, they miss it completely.</li>
                            <li class="fragment fade-up"><strong>Explainability Gap:</strong> A probability score (e.g., "98% Pneumonia") is not actionable. Doctors need to know <em>where</em> the lesion is.</li>
                        </ul>
                    </div>
                </div>
            </section>

            <section>
                <h2>üí° The Solution: Generative AI</h2>
                <p>We flip the paradigm: <span class="highlight">Train ONLY on Healthy Data.</span></p>
                
                <div class="cols">
                    <div class="col">
                        <ol style="font-size: 0.9em;">
                            <li class="fragment">The model learns the <strong>"Manifold of Health"</strong> (distribution of normal lung anatomy).</li>
                            <li class="fragment">It becomes an expert at reconstructing healthy lungs.</li>
                            <li class="fragment">When fed a diseased X-ray, it <strong>fails to reconstruct the anomaly</strong> because it has never seen one.</li>
                        </ol>
                        <div class="fragment tech-note">
                            <strong>The Breakthrough:</strong><br>
                            We calculate the pixel-wise difference: <br>
                            <code>| Input - Reconstruction | = Anomaly Map</code><br>
                            This automatically highlights the tumor/infection without supervision.
                        </div>
                    </div>
                </div>
            </section>

            <section>
                <h2>üöÄ Results & Significance</h2>
                <div class="cols">
                    <div class="col">
                        <table style="font-size: 0.7em;">
                            <thead>
                                <tr>
                                    <th>Feature</th>
                                    <th>Our Approach (Unsupervised)</th>
                                    <th>Traditional AI (Supervised)</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Data Req</strong></td>
                                    <td class="highlight">Only healthy scans needed</td>
                                    <td>Requires expensive expert labeling</td>
                                </tr>

                                <tr>
                                    <td><strong>Bias Risk</strong></td>
                                    <td class="highlight">Lower</td>
                                    <td>High (depends on labeled classes)</td>
                                </tr>
                                <tr>
                                    <td><strong>Detection Capability</strong></td>
                                    <td class="highlight">Can detect unknown diseases</td>
                                    <td>Cannot detect unseen patterns</td>
                                </tr>
                                <tr>
                                    <td><strong>Explainability</strong></td>
                                    <td class="highlight">Heatmaps show where anomaly exists</td>
                                    <td>Only probability score</td>
                                </tr>
                                <tr>
                                    <td><strong>Clinical Trust</strong></td>
                                    <td class="highlight">Easier for radiologists to interpret</td>
                                    <td>Harder without visual cues</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
                
            </section>
            
            
            
            <section>
                <h2>üèóÔ∏è System Architecture</h2>
                <div class="img-box">
                    <img src="https://github.com/user-attachments/assets/6a10e15f-3943-492c-8492-ec5c6413e9b2" alt="System Architecture Diagram">
                </div>
                <p style="font-size: 0.6em;">The pipeline: Input -> Resize -> Model Inference -> Difference Calculation -> Heatmap Overlay</p>
            </section>

            <section>
                <h2>üß† Variational Autoencoder (VAE)</h2>
                <div class="cols">
                    <div class="col" style="flex: 1.5;">
                        <div class="img-box">
                            <img src="https://github.com/user-attachments/assets/3677f72b-a443-4952-b1e0-263c63f15532" alt="VAE Architecture">
                        </div>
                    </div>
                    <div class="col">
                        <h4>Why VAE?</h4>
                        <ul style="font-size: 0.7em;">
                            <li><strong>Probabilistic Latent Space:</strong> Unlike standard Autoencoders, VAEs map inputs to a Gaussian distribution ($\mu, \sigma$).</li>
                            <li><strong>Reparameterization Trick:</strong> Allows backpropagation through stochastic sampling ($z = \mu + \sigma \cdot \epsilon$).</li>
                            <li><strong>Loss Function:</strong> MSE (Reconstruction) + KL-Divergence (Regularization).</li>
                        </ul>
                        <p class="accent" style="font-size: 0.7em; margin-top:10px;">Result: Smooth, stable reconstructions but slightly blurry.</p>
                    </div>
                </div>
            </section>

            <section>
                <h2>üß† Generative Adversarial Network (GAN)</h2>
                <div class="cols">
                    <div class="col" style="flex: 1.5;">
                        <div class="img-box">
                            <img src="https://github.com/user-attachments/assets/463168d9-f5fd-446b-81ab-13f79ca7d2fc" alt="GAN Architecture">
                        </div>
                    </div>
                    <div class="col">
                        <h4>Why GAN?</h4>
                        <ul style="font-size: 0.7em;">
                            <li><strong>Adversarial Game:</strong> A Generator creates images, a Discriminator tries to spot fakes. They improve each other.</li>
                            <li><strong>PatchGAN Discriminator:</strong> Focuses on high-frequency details (texture/edges) rather than just pixel averages.</li>
                            <li><strong>LeakyReLU:</strong> Used to prevent "dying ReLU" gradients during training.</li>
                        </ul>
                        <p class="accent" style="font-size: 0.7em; margin-top:10px;">Result: Sharp, realistic details, but training is unstable.</p>
                    </div>
                </div>
            </section>

            <section>
                <h2>üß† Vision Transformer (ViT-AE)</h2>
                <div class="cols">
                    <div class="col" style="flex: 1.5;">
                        <div class="img-box">
                            <img src="https://github.com/user-attachments/assets/d93c0284-a83f-4393-9214-32b6b6b602dd" alt="ViT Architecture">
                        </div>
                    </div>
                    <div class="col">
                        <h4>Why ViT?</h4>
                        <ul style="font-size: 0.7em;">
                            <li><strong>Patch Embeddings:</strong> Images are split into 16x16 patches and flattened (like words in NLP).</li>
                            <li><strong>Self-Attention:</strong> The model learns global relationships (e.g., how the left lung relates to the right lung) regardless of distance.</li>
                            <li><strong>Positional Encoding:</strong> Adds spatial awareness to the patch sequence.</li>
                        </ul>
                        <p class="accent" style="font-size: 0.7em; margin-top:10px;">Result: Captures global structure well, but data-hungry.</p>
                    </div>
                </div>
            </section>

<section>
                <h2>üöÄ Project Impact</h2>
                
                <div class="fragment" style="
                    background: linear-gradient(135deg, rgba(66, 175, 250, 0.2) 0%, rgba(30, 30, 30, 0.8) 100%);
                    border: 2px solid #42affa;
                    border-radius: 20px;
                    padding: 30px;
                    margin: 20px auto;
                    box-shadow: 0 0 30px rgba(66, 175, 250, 0.3);
                    text-align: center;
                ">
                    <h3 style="color: #42affa; margin-bottom: 10px; font-size: 1.2em; text-transform: uppercase; letter-spacing: 3px;">Current Breakthrough</h3>
                    
                    <div style="font-size: 4em; font-weight: 800; color: #fff; line-height: 1;">
                        ~40% <span style="font-size: 0.4em; color: #aaa; vertical-align: middle;">Accuracy</span>
                    </div>
                    
                    <p style="font-size: 1.1em; color: #eee; margin-top: 15px; font-weight: 300;">
                        Achieved on <strong>Pneumonia Detection</strong> without <em>ever</em> showing the model a single disease image during training even with small architectures and minimal trainable parameters.
                    </p>
                    <p style="font-size: 1.1em; color: #eee; margin-top: 15px; font-weight: 300;">
                        Our preliminary results validate this methodology as a <strong>robust</strong>, automated framework for accurate chest X-ray anomaly detection, effectively identifying <em>infections</em> and <em>tumors</em> with high sensitivity to support <strong>clinical decision-making</strong>.
                        
                    </p>                    
                    <div style="margin-top: 15px;">
                        <span style="background: #42affa; color: #000; padding: 5px 15px; border-radius: 15px; font-size: 0.6em; font-weight: bold;">ZERO-SHOT LEARNING</span>
                    </div>
                </div>
            </section>
            <section>
                <h1>Thank You</h1>
                <p>Bringing Explainable, Generative AI to Healthcare.</p>
                <div style="margin-top: 50px; border: 2px solid var(--primary); padding: 20px; border-radius: 15px; display: inline-block;">
                    <p style="margin:0; font-size: 0.7em; color: #888;">Explore the Code</p>
                    <a href="https://github.com/hzindia/NeuroScan" style="font-size: 1.2em; font-weight: bold; color: var(--primary);">github.com/hzindia/NeuroScan</a>
                </div>
            </section>

        </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/reveal.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/plugin/highlight/highlight.min.js"></script>
    <script>
        Reveal.initialize({
            hash: true,
            slideNumber: 'c/t',
            transition: 'convex',
            plugins: [ RevealHighlight ]
        });
    </script>
</body>
</html>
